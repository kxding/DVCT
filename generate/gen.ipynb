{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingkaixin/miniconda3/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from inversion_free import *\n",
    "from utils import *\n",
    "from transformers.models.clip.modeling_clip import CLIPTextModel\n",
    "from transformers import CLIPTokenizer\n",
    "from diffusers import DDIMScheduler, StableDiffusionPipeline\n",
    "from multi_token_clip import MultiTokenCLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(\n",
    "        pretrained_model_name_or_path,\n",
    "        seed=0,\n",
    "):\n",
    "    # backward_embeds_dict_name = \"%s.bin\" % str(train_step)\n",
    "    # backward_placeholder_token = \"<%s>\" % prompt_name_backward\n",
    "    # backward_embeds_dict_path = os.path.join(model_id, backward_embeds_dict_name)\n",
    "    # backward_learned_embeds_dict = torch.load(backward_embeds_dict_path)\n",
    "    # get reference embedding\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\",\n",
    "                                                 revision=False)\n",
    "    tokenizer = MultiTokenCLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "\n",
    "    # load_multitoken_tokenizer(tokenizer, text_encoder, backward_learned_embeds_dict, backward_placeholder_token)\n",
    "    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False,\n",
    "                              set_alpha_to_one=False)\n",
    "    ldm_stable = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path, scheduler=scheduler,\n",
    "                                                         tokenizer=tokenizer, text_encoder=text_encoder).to(device)\n",
    "    ldm_stable.safety_checker = lambda images, clip_input: (images, False)\n",
    "\n",
    "    generator = torch.Generator(device=device)\n",
    "    if seed is not None:\n",
    "        generator.manual_seed(seed)\n",
    "\n",
    "    # controller arguments\n",
    "\n",
    "    return generator, ldm_stable, device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CLIPTokenizer'. \n",
      "The class this function is called from is 'MultiTokenCLIPTokenizer'.\n",
      "/home/dingkaixin/miniconda3/envs/ldm/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "The config attributes {'dropout': 0.0} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/home/dingkaixin/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:101: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.10.1\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": false,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": false,\n",
      "  \"steps_offset\": 0,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "#将 model_path 设置为 Stable-Diffusion-v1.5 的模型路径或名字\n",
    "model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "# model_path = \"../../s\"\n",
    "generator, model, device = init_model(model_path)\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_path, subfolder=\"text_encoder\",\n",
    "                                                 revision=False)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_path, subfolder=\"tokenizer\")\n",
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", \n",
    "    clip_sample=False, set_alpha_to_one=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一些路径和参数\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "# 加载图片\n",
    "# 第一级目录\n",
    "img_dir_src_1 = \"./examples/\"\n",
    "img_dir_tar_1 = img_dir_src_1\n",
    "# 第二级目录\n",
    "img_dir_src_2 = \"dog\"\n",
    "src_img_dir = os.path.join(img_dir_src_1, img_dir_src_2)\n",
    "tar_img_dir_2 = \"dog_hat\"\n",
    "tar_img_dir = os.path.join(img_dir_tar_1, tar_img_dir_2)\n",
    "\n",
    "def get_image_file(path):\n",
    "    img_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    img_files = [f for f in os.listdir(path) if f.lower().endswith(img_extensions)]\n",
    "    return img_files[0]\n",
    "\n",
    "src_img_file = get_image_file(src_img_dir)\n",
    "tar_img_file = get_image_file(tar_img_dir)\n",
    "\n",
    "src_latent = img2latent(os.path.join(src_img_dir, src_img_file), model, device)\n",
    "tar_latent = img2latent(os.path.join(tar_img_dir, tar_img_file), model, device)\n",
    "\n",
    "# 加载嵌入向量\n",
    "# 第一级目录\n",
    "embed_dir_src_1 = \"./output/\"\n",
    "embed_dir_tar_1 = embed_dir_src_1\n",
    "\n",
    "# 第二级目录\n",
    "embed_dir_src_2 = \"dog\"\n",
    "embed_dir_src_2 = os.path.join(embed_dir_src_1, embed_dir_src_2)\n",
    "tar_embed_dir_2 = \"dog_hat\"\n",
    "tar_embed_dir_2 = os.path.join(embed_dir_tar_1, tar_embed_dir_2)\n",
    "\n",
    "# 嵌入向量名字\n",
    "src_embed_dir = \"dog_05_08_2024_1919\"\n",
    "src_embed_dir = os.path.join(embed_dir_src_2, src_embed_dir)\n",
    "tar_embed_dir = \"dog_hat_05_08_2024_2000\"\n",
    "tar_embed_dir = os.path.join(tar_embed_dir_2, tar_embed_dir)\n",
    "# 嵌入向量训练步数\n",
    "src_steps = 1000\n",
    "tar_steps = src_steps\n",
    "\n",
    "src_embedding = torch.load(os.path.join(src_embed_dir, f\"{src_steps}.bin\")).to(device)\n",
    "tar_embedding = torch.load(os.path.join(tar_embed_dir, f\"{tar_steps}.bin\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一些路径和参数\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "# 加载图片\n",
    "# 第一级目录\n",
    "img_dir_src_1 = \"./examples/\"\n",
    "img_dir_tar_1 = img_dir_src_1\n",
    "# 第二级目录\n",
    "img_dir_src_2 = \"dog\"\n",
    "src_img_dir = os.path.join(img_dir_src_1, img_dir_src_2)\n",
    "tar_img_dir_2 = \"dog_hat\"\n",
    "tar_img_dir = os.path.join(img_dir_tar_1, tar_img_dir_2)\n",
    "\n",
    "def get_image_file(path):\n",
    "    img_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    img_files = [f for f in os.listdir(path) if f.lower().endswith(img_extensions)]\n",
    "    return img_files[0]\n",
    "\n",
    "src_img_file = get_image_file(src_img_dir)\n",
    "tar_img_file = get_image_file(tar_img_dir)\n",
    "\n",
    "src_latent = img2latent(os.path.join(src_img_dir, src_img_file), model, device)\n",
    "tar_latent = img2latent(os.path.join(tar_img_dir, tar_img_file), model, device)\n",
    "\n",
    "# 加载嵌入向量\n",
    "# 第一级目录\n",
    "embed_dir_src_1 = \"./output/\"\n",
    "embed_dir_tar_1 = embed_dir_src_1\n",
    "\n",
    "# 第二级目录\n",
    "embed_dir_src_2 = \"dog\"\n",
    "embed_dir_src_2 = os.path.join(embed_dir_src_1, embed_dir_src_2)\n",
    "tar_embed_dir_2 = \"dog_hat\"\n",
    "tar_embed_dir_2 = os.path.join(embed_dir_tar_1, tar_embed_dir_2)\n",
    "\n",
    "# 嵌入向量名字\n",
    "src_embed_dir = \"dog_05_08_2024_1919\"\n",
    "src_embed_dir = os.path.join(embed_dir_src_2, src_embed_dir)\n",
    "tar_embed_dir = \"dog_hat_05_08_2024_2000\"\n",
    "tar_embed_dir = os.path.join(tar_embed_dir_2, tar_embed_dir)\n",
    "# 嵌入向量训练步数\n",
    "src_steps = 1000\n",
    "tar_steps = src_steps\n",
    "\n",
    "src_embedding = torch.load(os.path.join(src_embed_dir, f\"{src_steps}.bin\")).to(device)\n",
    "tar_embedding = torch.load(os.path.join(tar_embed_dir, f\"{tar_steps}.bin\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 23-22-17_dog2dog_hat_30_steps_src0_tar1.png in ./output_img/2024-05-09-dog-to-dog_hat/\n"
     ]
    }
   ],
   "source": [
    "# 设置输出目录并生成\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_dir = f\"./output_img/{str(date)}-{img_dir_src_2}-to-{tar_img_dir_2}/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "num_inference_steps = 30\n",
    "save_all = False\n",
    "\n",
    "# src 和 tar 方向的系数\n",
    "src_weight = 0\n",
    "tar_weight = 1\n",
    "\n",
    "output_img_name = f\"{time}_{src_img_file[:-4]}2{tar_img_file[:-4]}_{num_inference_steps}_steps_src{src_weight}_tar{tar_weight}\"\n",
    "\n",
    "# 生成图片\n",
    "# src_latent = tar_latent\n",
    "latents = gen_inversion_free(model, \n",
    "                             src_latent, \n",
    "                             tar_latent, \n",
    "                             src_embedding, \n",
    "                             tar_embedding, \n",
    "                            num_inference_steps=num_inference_steps, \n",
    "                            tar_weight=tar_weight,  src_weight=src_weight,\n",
    ")\n",
    "\n",
    "if save_all:\n",
    "    for i, latent in enumerate(latents):\n",
    "        img = latent2img(latent.detach(), model)\n",
    "        print(f\"Saving {output_img_name}_{i}.png in {output_dir}\")\n",
    "        img.save(os.path.join(output_dir, f\"{output_img_name}_{i}.png\"))\n",
    "else:\n",
    "    img = latent2img(latents.detach(), model)\n",
    "    print(f\"Saving {output_img_name}.png in {output_dir}\")\n",
    "    img.save(os.path.join(output_dir, f\"{output_img_name}.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
