{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-09T18:51:28.297266Z",
     "start_time": "2024-05-09T18:51:25.680479Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPTextModel\n",
    "from diffusers import DDIMScheduler\n",
    "from multi_token_clip import MultiTokenCLIPTokenizer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CLIPTokenizer'. \n",
      "The class this function is called from is 'MultiTokenCLIPTokenizer'.\n",
      "D:\\anaconda\\lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion.py:101: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.10.1\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": false,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": false,\n",
      "  \"steps_offset\": 0,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "# 将 model_path 设置为 Stable-Diffusion-v1.5 的模型路径或名字\n",
    "model_or_name = \"../../model/\"\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_or_name, subfolder=\"text_encoder\", revision=False)\n",
    "tokenizer = MultiTokenCLIPTokenizer.from_pretrained(model_or_name, subfolder=\"tokenizer\")\n",
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", \n",
    "    clip_sample=False, set_alpha_to_one=False\n",
    ")\n",
    "model = StableDiffusionPipeline.from_pretrained(\n",
    "    model_or_name, scheduler=scheduler, \n",
    "    text_encoder=text_encoder, tokenizer=tokenizer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T18:52:36.777192Z",
     "start_time": "2024-05-09T18:51:28.298405Z"
    }
   },
   "id": "83810ef29e3285d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 设置一些路径和参数\n",
    "from inversion_free import gen_inversion_free\n",
    "from utils import img2latent, latent2img\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 加载图片\n",
    "# 第一级目录\n",
    "img_dir_src_1 = \"./pair/\"\n",
    "img_dir_tar_1 = img_dir_src_1\n",
    "# 第二级目录\n",
    "img_dir_src_2 = \"horse\"\n",
    "src_img_dir = os.path.join(img_dir_src_1, img_dir_src_2)\n",
    "tar_img_dir_2 = \"zebra\"\n",
    "tar_img_dir = os.path.join(img_dir_tar_1, tar_img_dir_2)\n",
    "\n",
    "def get_image_file(path):\n",
    "    img_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    img_files = [f for f in os.listdir(path) if f.lower().endswith(img_extensions)]\n",
    "    return img_files[0]\n",
    "\n",
    "src_img_file = get_image_file(src_img_dir)\n",
    "tar_img_file = get_image_file(tar_img_dir)\n",
    "\n",
    "src_latent = img2latent(os.path.join(src_img_dir, src_img_file), model)\n",
    "tar_latent = img2latent(os.path.join(tar_img_dir, tar_img_file), model)\n",
    "\n",
    "# 加载嵌入向量\n",
    "# 第一级目录\n",
    "embed_dir_src_1 = \"./output/\"\n",
    "embed_dir_tar_1 = embed_dir_src_1\n",
    "\n",
    "# 第二级目录\n",
    "embed_dir_src_2 = \"hrs\"\n",
    "embed_dir_src_2 = os.path.join(embed_dir_src_1, embed_dir_src_2)\n",
    "tar_embed_dir_2 = \"zbr\"\n",
    "tar_embed_dir_2 = os.path.join(embed_dir_tar_1, tar_embed_dir_2)\n",
    "\n",
    "# 嵌入向量名字\n",
    "src_embed_dir = \"horse_04_25_2024_1603\"\n",
    "src_embed_dir = os.path.join(embed_dir_src_2, src_embed_dir)\n",
    "tar_embed_dir = \"zebra_04_25_2024_1557\"\n",
    "tar_embed_dir = os.path.join(tar_embed_dir_2, tar_embed_dir)\n",
    "# 嵌入向量训练步数\n",
    "src_steps = 100\n",
    "tar_steps = src_steps\n",
    "\n",
    "src_embedding = torch.load(os.path.join(src_embed_dir, f\"{src_steps}.bin\"))\n",
    "tar_embedding = torch.load(os.path.join(tar_embed_dir, f\"{tar_steps}.bin\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T18:52:42.704671Z",
     "start_time": "2024-05-09T18:52:36.778282Z"
    }
   },
   "id": "4a46f549390fc345",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [07:17<00:00, 23.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 02-52-42_horse2zebra_20steps_none-src_cos_cfg0.7_src1_tar1.png in ./output_img/2024-05-10-horse-to-zebra/\n"
     ]
    }
   ],
   "source": [
    "# 设置输出目录并生成\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_dir = f\"./output_img/{str(date)}-{img_dir_src_2}-to-{tar_img_dir_2}/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "num_inference_steps = 20\n",
    "save_all = False\n",
    "\n",
    "# 选择权重的类型\n",
    "inclination = \"none-src\"\n",
    "mode = \"cosine\"\n",
    "cfg_guidance_scale = 0.7\n",
    "\n",
    "# src 和 tar 方向的系数\n",
    "src_coefficient = 1\n",
    "tar_coefficient = 1\n",
    "\n",
    "output_img_name = f\"{time}_{src_img_file[:-4]}2{tar_img_file[:-4]}_{num_inference_steps}steps_{inclination}_{mode[:3]}_cfg{cfg_guidance_scale}_src{src_coefficient}_tar{tar_coefficient}\"\n",
    "\n",
    "# 生成图片\n",
    "latents = gen_inversion_free(\n",
    "    model.to(device), src_latent, tar_latent, src_embedding, tar_embedding,\n",
    "    num_inference_steps=num_inference_steps, mode=mode, inclination=inclination,\n",
    "    cfg_guidance=cfg_guidance_scale, src_coef=src_coefficient, tar_coef=tar_coefficient,\n",
    ")\n",
    "\n",
    "\n",
    "if save_all:\n",
    "    for i, latent in enumerate(latents):\n",
    "        img = latent2img(latent.detach(), model)\n",
    "        print(f\"Saving {output_img_name}_{i}.png in {output_dir}\")\n",
    "        img.save(os.path.join(output_dir, f\"{output_img_name}_{i}.png\"))\n",
    "else:\n",
    "    img = latent2img(latents[-1].detach(), model)\n",
    "    print(f\"Saving {output_img_name}.png in {output_dir}\")\n",
    "    img.save(os.path.join(output_dir, f\"{output_img_name}.png\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T19:00:46.441219Z",
     "start_time": "2024-05-09T18:52:42.705751Z"
    }
   },
   "id": "d163cc670ff39824"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
