{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inversion_free import *\n",
    "from utils import *\n",
    "from transformers.models.clip.modeling_clip import CLIPTextModel\n",
    "from diffusers import DDIMScheduler, StableDiffusionPipeline\n",
    "from multi_token_clip import MultiTokenCLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(\n",
    "        pretrained_model_name_or_path,\n",
    "        seed=0,\n",
    "):\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\",\n",
    "                                                 revision=False)\n",
    "    tokenizer = MultiTokenCLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "\n",
    "    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False,\n",
    "                              set_alpha_to_one=False)\n",
    "    ldm_stable = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path, scheduler=scheduler,\n",
    "                                                         tokenizer=tokenizer, text_encoder=text_encoder).to(device)\n",
    "    ldm_stable.safety_checker = lambda images, clip_input: (images, False)\n",
    "\n",
    "    generator = torch.Generator(device=device)\n",
    "    if seed is not None:\n",
    "        generator.manual_seed(seed)\n",
    "\n",
    "    return generator, ldm_stable, device, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将 model_path 设置为 Stable-Diffusion-v1.5 的模型路径或名字\n",
    "model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "# model_path = \"../../s\"\n",
    "generator, model, device, tokenizer = init_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一些路径和参数\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "# 加载图片\n",
    "# 第一级目录\n",
    "img_dir_src_1 = \"./examples/\"\n",
    "img_dir_tar_1 = img_dir_src_1\n",
    "# 第二级目录\n",
    "img_dir_src_2 = \"dog\"\n",
    "src_img_dir = os.path.join(img_dir_src_1, img_dir_src_2)\n",
    "tar_img_dir_2 = \"dog_hat\"\n",
    "tar_img_dir = os.path.join(img_dir_tar_1, tar_img_dir_2)\n",
    "\n",
    "\n",
    "def get_image_file(path):\n",
    "    img_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    img_files = [f for f in os.listdir(path) if f.lower().endswith(img_extensions)]\n",
    "    return img_files[0]\n",
    "\n",
    "\n",
    "src_img_file = get_image_file(src_img_dir)\n",
    "tar_img_file = get_image_file(tar_img_dir)\n",
    "\n",
    "src_latent = img2latent(os.path.join(src_img_dir, src_img_file), model, device)\n",
    "tar_latent = img2latent(os.path.join(tar_img_dir, tar_img_file), model, device)\n",
    "\n",
    "# 加载嵌入向量\n",
    "# 第一级目录\n",
    "embed_dir_src_1 = \"./output/\"\n",
    "embed_dir_tar_1 = embed_dir_src_1\n",
    "\n",
    "# 第二级目录\n",
    "embed_dir_src_2 = \"dog\"\n",
    "embed_dir_src_2 = os.path.join(embed_dir_src_1, embed_dir_src_2)\n",
    "tar_embed_dir_2 = \"dog_hat\"\n",
    "tar_embed_dir_2 = os.path.join(embed_dir_tar_1, tar_embed_dir_2)\n",
    "\n",
    "# 嵌入向量名字\n",
    "src_embed_dir = \"dog_05_08_2024_1919\"\n",
    "src_embed_dir = os.path.join(embed_dir_src_2, src_embed_dir)\n",
    "tar_embed_dir = \"dog_hat_05_08_2024_2000\"\n",
    "tar_embed_dir = os.path.join(tar_embed_dir_2, tar_embed_dir)\n",
    "# 嵌入向量训练步数\n",
    "src_steps = 1000\n",
    "tar_steps = src_steps\n",
    "\n",
    "src_embedding = torch.load(os.path.join(src_embed_dir, f\"{src_steps}.bin\")).to(device)\n",
    "tar_embedding = torch.load(os.path.join(tar_embed_dir, f\"{tar_steps}.bin\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_control import make_controller\n",
    "\n",
    "# 设置输出目录并生成\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_dir = f\"./output_img/{str(date)}-{img_dir_src_2}-to-{tar_img_dir_2}/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "num_inference_steps = 30\n",
    "save_all = False\n",
    "use_attention = True\n",
    "\n",
    "# src 和 tar 方向的系数\n",
    "src_weight = 0\n",
    "tar_weight = 1\n",
    "\n",
    "output_img_name = f\"{time}_{src_img_file[:-4]}2{tar_img_file[:-4]}_{num_inference_steps}_steps_src{src_weight}_tar{tar_weight}\"\n",
    "\n",
    "if use_attention:\n",
    "    # 设置注意力控制器\n",
    "    placeholder = [\"\", \"<s2>\"]\n",
    "    cross_injection_ratio = 0.\n",
    "    self_injection_ratio = 0.7\n",
    "    eq_param = {\n",
    "        'words': (placeholder[-1],),\n",
    "        'values': (0.5,),\n",
    "    }\n",
    "    controller = make_controller(\n",
    "        prompts=placeholder,\n",
    "        tokenizer=tokenizer,\n",
    "        is_replace_controller=False,\n",
    "        cross_replace_steps={'default_': cross_injection_ratio},\n",
    "        self_replace_steps=self_injection_ratio,\n",
    "        equilizer_params=eq_param,\n",
    "    )\n",
    "else:\n",
    "    controller = None\n",
    "\n",
    "# 生成图片\n",
    "# src_latent = tar_latent\n",
    "latents = gen_inversion_free(model,\n",
    "                             src_latent,\n",
    "                             tar_latent,\n",
    "                             src_embedding,\n",
    "                             tar_embedding,\n",
    "                             num_inference_steps=num_inference_steps,\n",
    "                             tar_weight=tar_weight, src_weight=src_weight,\n",
    "                             )\n",
    "\n",
    "if save_all:\n",
    "    for i, latent in enumerate(latents):\n",
    "        img = latent2img(latent.detach(), model)\n",
    "        print(f\"Saving {output_img_name}_{i}.png in {output_dir}\")\n",
    "        img.save(os.path.join(output_dir, f\"{output_img_name}_{i}.png\"))\n",
    "else:\n",
    "    img = latent2img(latents[-1].detach(), model)\n",
    "    print(f\"Saving {output_img_name}.png in {output_dir}\")\n",
    "    img.save(os.path.join(output_dir, f\"{output_img_name}.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
