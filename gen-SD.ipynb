{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:58:51.662471Z",
     "start_time": "2024-05-13T16:58:48.131307Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\vct\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from diffusers import LCMScheduler\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00a0ca7b718c42e1ab755fcd99e64867"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 将 model_path 设置为 Stable-Diffusion-v1.5 的模型路径或名字\n",
    "model_or_name = \"F:/SRTP/model\"\n",
    "\n",
    "scheduler = LCMScheduler.from_pretrained(model_or_name, subfolder=\"scheduler\")\n",
    "model = StableDiffusionPipeline.from_pretrained(\n",
    "    model_or_name, scheduler=scheduler,\n",
    ")\n",
    "\n",
    "# 设置随机种子\n",
    "seed = 0\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "\n",
    "model.generator = generator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:59:00.187543Z",
     "start_time": "2024-05-13T16:58:51.663647Z"
    }
   },
   "id": "83810ef29e3285d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 设置一些路径和参数\n",
    "from inversion_free import gen_inversion_free\n",
    "from utils import img2latent, latent2img\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# 加载图片\n",
    "# 第一级目录\n",
    "img_dir_src_1 = \"./samples/\"\n",
    "img_dir_tar_1 = img_dir_src_1\n",
    "# 第二级目录\n",
    "img_dir_src_2 = \"mount\"\n",
    "src_img_dir = os.path.join(img_dir_src_1, img_dir_src_2)\n",
    "tar_img_dir_2 = \"van\"\n",
    "tar_img_dir = os.path.join(img_dir_tar_1, tar_img_dir_2)\n",
    "\n",
    "def get_image_file(path):\n",
    "    img_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    img_files = [f for f in os.listdir(path) if f.lower().endswith(img_extensions)]\n",
    "    return img_files[0]\n",
    "\n",
    "src_img_file = get_image_file(src_img_dir)\n",
    "tar_img_file = get_image_file(tar_img_dir)\n",
    "\n",
    "src_latent = img2latent(os.path.join(src_img_dir, src_img_file), model)\n",
    "tar_latent = img2latent(os.path.join(tar_img_dir, tar_img_file), model)\n",
    "\n",
    "# 加载嵌入向量\n",
    "# 第一级目录\n",
    "embed_dir_src_1 = \"./output/\"\n",
    "embed_dir_tar_1 = embed_dir_src_1\n",
    "\n",
    "# 第二级目录\n",
    "embed_dir_src_2 = \"mnt\"\n",
    "embed_dir_src_2 = os.path.join(embed_dir_src_1, embed_dir_src_2)\n",
    "tar_embed_dir_2 = \"van\"\n",
    "tar_embed_dir_2 = os.path.join(embed_dir_tar_1, tar_embed_dir_2)\n",
    "\n",
    "# 嵌入向量名字\n",
    "src_embed_dir = \"mount_04_25_2024_1544\"\n",
    "src_embed_dir = os.path.join(embed_dir_src_2, src_embed_dir)\n",
    "tar_embed_dir = \"van_04_25_2024_1539\"\n",
    "tar_embed_dir = os.path.join(tar_embed_dir_2, tar_embed_dir)\n",
    "\n",
    "# 嵌入向量训练步数\n",
    "src_steps = 100\n",
    "tar_steps = src_steps\n",
    "\n",
    "src_embedding = torch.load(os.path.join(src_embed_dir, f\"{src_steps}.bin\"))\n",
    "tar_embedding = torch.load(os.path.join(tar_embed_dir, f\"{tar_steps}.bin\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:59:05.185037Z",
     "start_time": "2024-05-13T16:59:00.188680Z"
    }
   },
   "id": "4a46f549390fc345",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CLIPTokenizer'. \n",
      "The class this function is called from is 'MultiTokenCLIPTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of placeholder tokens are: 3\n",
      "number of placeholder tokens are: 3\n"
     ]
    }
   ],
   "source": [
    "from utils import load_multitoken_tokenizer\n",
    "from multi_token_clip import MultiTokenCLIPTokenizer\n",
    "from transformers.models.clip.modeling_clip import CLIPTextModel\n",
    "\n",
    "src_placeholders = \"<src>\"\n",
    "tar_placeholders = \"<tar>\"\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_or_name, subfolder=\"text_encoder\", revision=False)\n",
    "tokenizer = MultiTokenCLIPTokenizer.from_pretrained(model_or_name, subfolder=\"tokenizer\")\n",
    "\n",
    "placeholder_dict = {\n",
    "    \"<src>\": src_embedding[0][:3],\n",
    "    \"<tar>\": tar_embedding[0][:3],\n",
    "}\n",
    "\n",
    "load_multitoken_tokenizer(tokenizer, text_encoder, placeholder_dict, tar_placeholders)\n",
    "load_multitoken_tokenizer(tokenizer, text_encoder, placeholder_dict, src_placeholders)\n",
    "\n",
    "model.text_encoder = text_encoder\n",
    "model.tokenizer = tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:59:06.816400Z",
     "start_time": "2024-05-13T16:59:05.186047Z"
    }
   },
   "id": "73d9d810e9b79d4f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src weight\n",
      "tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000])\n",
      "tar weight\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]D:\\anaconda\\envs\\vct\\lib\\site-packages\\diffusers\\models\\attention_processor.py:1279: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  hidden_states = F.scaled_dot_product_attention(\n",
      "100%|██████████| 20/20 [06:04<00:00, 18.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 00-59-06_mount2van_20steps_none-tar_cos_cfg0.7_src0.6_tar0_attnFalse.png in ./output_img/2024-05-14-mount-to-van/\n",
      "src weight\n",
      "tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000])\n",
      "tar weight\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [06:21<00:00, 19.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 00-59-06_mount2van_20steps_none-tar_cos_cfg1.7_src0.6_tar0_attnFalse.png in ./output_img/2024-05-14-mount-to-van/\n",
      "src weight\n",
      "tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000])\n",
      "tar weight\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [06:03<00:00, 18.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 00-59-06_mount2van_20steps_none-tar_cos_cfg2.7_src0.6_tar0_attnFalse.png in ./output_img/2024-05-14-mount-to-van/\n"
     ]
    }
   ],
   "source": [
    "from attention_control import make_controller\n",
    "\n",
    "# 设置输出目录并生成\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_dir = f\"./output_img/{str(date)}-{img_dir_src_2}-to-{tar_img_dir_2}/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "num_inference_steps = 20\n",
    "save_all = False\n",
    "\n",
    "for attn in [False]:\n",
    "    for cfg in [0.7, 1.7, 2.7]:\n",
    "        # 是否使用注意力控制器\n",
    "        use_attention = attn\n",
    "        \n",
    "        # 选择权重的类型\n",
    "        inclination = \"none-tar\"\n",
    "        mode = \"cosine\"\n",
    "        cfg_guidance_scale = cfg\n",
    "        \n",
    "        # src 和 tar 方向的系数\n",
    "        src_coefficient = 0.6\n",
    "        tar_coefficient = 0\n",
    "        \n",
    "        output_img_name = f\"{time}_{src_img_file[:-4]}2{tar_img_file[:-4]}_{num_inference_steps}steps_{inclination}_{mode[:3]}_cfg{cfg_guidance_scale}_src{src_coefficient}_tar{tar_coefficient}_attn{use_attention}\"\n",
    "        \n",
    "        if use_attention:\n",
    "            # 设置注意力控制器\n",
    "            placeholder = [src_placeholders, tar_placeholders]\n",
    "            cross_injection_ratio = 0.2\n",
    "            self_injection_ratio = 0.7\n",
    "            eq_param = {\n",
    "                'words' : (placeholder[-1],),\n",
    "                'values' : (0.5,),\n",
    "            }\n",
    "            controller = make_controller(\n",
    "                prompts=placeholder,\n",
    "                tokenizer=tokenizer,\n",
    "                is_replace_controller=False,\n",
    "                cross_replace_steps={'default_': cross_injection_ratio},\n",
    "                self_replace_steps=self_injection_ratio,\n",
    "                equilizer_params=eq_param,\n",
    "            )\n",
    "            model_2 = None\n",
    "        else:\n",
    "            controller = None\n",
    "            model_2 = None\n",
    "            \n",
    "        # 生成图片\n",
    "        latents = gen_inversion_free(\n",
    "            model.to(device), src_latent, tar_latent, src_embedding, tar_embedding,\n",
    "            num_inference_steps=num_inference_steps, mode=mode, inclination=inclination,\n",
    "            cfg_guidance=cfg_guidance_scale, src_coef=src_coefficient, tar_coef=tar_coefficient,\n",
    "            controller=controller, model_2=model_2, return_all=save_all,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        if save_all:\n",
    "            for i, latent in enumerate(latents):\n",
    "                img = latent2img(latent[0].detach(), model)\n",
    "                print(f\"Saving {output_img_name}_{i}.png in {output_dir}\")\n",
    "                img.save(os.path.join(output_dir, f\"{output_img_name}_{i}.png\"))\n",
    "        else:\n",
    "            img = latent2img(latents[-1].detach(), model)\n",
    "            print(f\"Saving {output_img_name}.png in {output_dir}\")\n",
    "            img.save(os.path.join(output_dir, f\"{output_img_name}.png\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T17:18:10.638964Z",
     "start_time": "2024-05-13T16:59:06.819660Z"
    }
   },
   "id": "d163cc670ff39824"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
